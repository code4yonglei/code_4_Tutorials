# Chapter 4: Feature Extraction

Previous examples use real-valued explanatory variables, such as height, weight, salary
- Many ML problems require learning from categorical variables, text, or images.
- In this chapter, we learn to create features that represent such variables.


## 4.1 Extracting features from categorical variables

Many problems have explanatory variables that are **categorical** or **nominal**.
- A categorical variable can take one of a fixed set of values
    - an application that predicts job salary might use categorical variables such as position in a city
- Categorical variables are commonly encoded using **one-of-k encoding**, or **one-hot encoding**, in which explanatory variable is represented using one binary feature for each of its possible values
    - assume model has a city variable: NY, SF, or LA
    - One-hot encoding represents variable using one binary feature for each city
    - Scikit-learn's `DictVectorizer` class is a transformer to one-hot encode categorical features
    ```
    from sklearn.feature_extraction import DictVectorizer
    one_hot_encoder = DictVectorizer()

    cities = [{'city': 'LA'}, {'city': 'NY'}, {'city': 'SF'}]
    print(cities)
    print(one_hot_encoder.fit_transform(cities).toarray())
    ```
- It may seem intuitive to represent values of a categorical explanatory variable with a single integer feature
    - LA by 0, NY by 1 and SF by 2
    - problem is that this representation encodes artificial information.
    - pepresenting cities with integers encodes an order for cities that does not exist in real world, and facilitates comparisons of them that do not make sense
    - there is no natural order of cities by which SF is 1 more than NY.
    - One-hot encoding avoids this problem and only represents variable value


## 4.2 Standardizing features

We already know that many learning algorithms perform better when they are trained on standardized data.
- standardized data has zero mean and unit variance
- An explanatory variable with zero mean is centered about origin; its average value is zero
- A feature vector has unit variance when variances of its features are all of same order of magnitude
- If one feature's variance is orders of magnitude greater than variances of other features, that feature may dominate learning algorithm and prevent it from learning from other variables (**tutorial with height in mm, cm, or m units**)
- Some learning algorithms also converge to optimal parameter values more slowly when data is not standardized

In addition to `StandardScaler` transformer, `scale` function from `preprocessing` module can be used to standardize a dataset along any axis.
```
from sklearn import preprocessing
import numpy as np

array = np.array([
    [0., 0., 5., 13., 9., 1.],
    [0., 0., 13., 15., 10., 15.],
    [0., 3., 15., 2., 0., 11.]
])
print(preprocessing.scale(array))
print(preprocessing.scale(array, axis=1))
```

`RobustScaler` is an alternative to `StandardScaler` that is robust to outliers.
- `StandardScaler` subtracts mean of a feature from each instance's value, and divides by feature's standard deviation
- `RobustScaler` subtracts median and divides by **interquartile range** so as to mitigate effect of large outliers
- Quartiles are calculated by splitting sorted dataset into 4 parts of equal size. median is 2nd quartile; interquartile range is the difference of 3rd and 1st quartiles






:::danger
:::
