# Chapter 4: Feature Extraction

Previous examples use real-valued explanatory variables, such as height, weight, salary
- Many ML problems require learning from categorical variables, text, or images.
- In this chapter, we learn to create features that represent such variables.


## 4.1 Extracting features from categorical variables

Many problems have explanatory variables that are **categorical** or **nominal**.
- A categorical variable can take one of a fixed set of values
    - an application that predicts job salary might use categorical variables such as position in a city
- Categorical variables are commonly encoded using **one-of-k encoding**, or **one-hot encoding**, in which explanatory variable is represented using one binary feature for each of its possible values
    - assume model has a city variable: NY, SF, or LA
    - One-hot encoding represents variable using one binary feature for each city
    - Scikit-learn's `DictVectorizer` class is a transformer to one-hot encode categorical features
    ```
    from sklearn.feature_extraction import DictVectorizer
    one_hot_encoder = DictVectorizer()

    cities = [{'city': 'LA'}, {'city': 'NY'}, {'city': 'SF'}]
    print(cities)
    print(one_hot_encoder.fit_transform(cities).toarray())
    ```
- It may seem intuitive to represent values of a categorical explanatory variable with a single integer feature
    - LA by 0, NY by 1 and SF by 2
    - problem is that this representation encodes artificial information.
    - pepresenting cities with integers encodes an order for cities that does not exist in real world, and facilitates comparisons of them that do not make sense
    - there is no natural order of cities by which SF is 1 more than NY.
    - One-hot encoding avoids this problem and only represents variable value


## 4.2 Standardizing features

We already know that many learning algorithms perform better when they are trained on standardized data.
- standardized data has zero mean and unit variance
- An explanatory variable with zero mean is centered about origin; its average value is zero
- A feature vector has unit variance when variances of its features are all of same order of magnitude
- If one feature's variance is orders of magnitude greater than variances of other features, that feature may dominate learning algorithm and prevent it from learning from other variables (**tutorial with height in mm, cm, or m units**)
- Some learning algorithms also converge to optimal parameter values more slowly when data is not standardized

In addition to `StandardScaler` transformer, `scale` function from `preprocessing` module can be used to standardize a dataset along any axis.
```
from sklearn import preprocessing
import numpy as np

array = np.array([
    [0., 0., 5., 13., 9., 1.],
    [0., 0., 13., 15., 10., 15.],
    [0., 3., 15., 2., 0., 11.]
])
print(preprocessing.scale(array))
print(preprocessing.scale(array, axis=1))
```

`RobustScaler` is an alternative to `StandardScaler` that is robust to outliers.
- `StandardScaler` subtracts mean of a feature from each instance's value, and divides by feature's standard deviation
- `RobustScaler` subtracts median and divides by **interquartile range** so as to mitigate effect of large outliers
- Quartiles are calculated by splitting sorted dataset into 4 parts of equal size. median is 2nd quartile; interquartile range is the difference of 3rd and 1st quartiles


## 4.3 Extracting features from text

> Many ML problems use text, which usually represents natural language. Text must be transformed to a vector representation that encodes some aspect of its meaning.
> In following sections, we will review variations of two most common representation of text that are used in ML: bag-of-words model and word embeddings.


### 4.3.1 Bag-of-words model

The most common representation of text is **bag-of-words model**.
- This representation uses a multiset, or bag, that encodes words that appear in a text; bag-of-words does not encode any of text's syntax, ignores order of words, and disregards all grammar.
- Bag-of-words can be thought of as **an extension to one-hot encoding**. It creates one feature for each word of interest in text.
- The bag-of-words model is motivated by intuition that documents containing similar words often have similar meanings.
- The bag-of-words model can be used effectively for document classification and retrieval despite limited information that it encodes.
- A collection of documents is called a **corpus**.

Here we use a corpus with following two documents to examine bag-of-words model:
```
corpus = [
    'UNC played Duke in basketball',
    'Duke lost the basketball game'
]
```
- this corpus contains eight unique words, and these unique words comprise its vocabulary.
- The bag-of-words model uses a feature vector with an element for each word in corpus's vocabulary to represent each document. 
- Our corpus has eight unique words, so each document will be represented by a vector with eight elements.
- The number of elements that comprise a feature vector is called the **vector's dimension**.
- A dictionary maps vocabulary to indices in feature vector.
    - The dictionary for a bag-of-words could be implemented using a Python `Dictionary`, but Python data structure and representation's mapping are distinct.




:::danger
:::
