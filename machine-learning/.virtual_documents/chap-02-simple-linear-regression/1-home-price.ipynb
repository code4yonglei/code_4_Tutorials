import numpy as np
import pandas as pd
import matplotlib.pyplot as plt





df1 = pd.read_csv("1-home-price-train.csv")
df1


plt.figure()
plt.title("House price plotted against house size")
plt.scatter(df1.area, df1.price, color="red", marker='o')
plt.xlabel("House area (square feet)")
plt.ylabel("House price (*1000 USD)")
plt.grid(True)

plt.show()
# plt.savefig("./1-house-price-0.png")





from sklearn.linear_model import LinearRegression


model = LinearRegression()
model.fit(df1[["area"]].values, df1.price)


# fitting parameters
print(model.coef_, model.intercept_)


# accuracy of model
print(model.score(df1[["area"]].values, df1.price))


print(np.mean((model.predict(df1[["area"]]) - df1.price)**2))


%matplotlib inline

plt.figure()
plt.title("House price plotted against house size")
plt.xlabel("House area (square feet)")
plt.ylabel("House price (*1000 USD)")
plt.grid(True)

plt.scatter(df1.area, df1.price, color="red", marker='o', label="Training data")
plt.plot(df1.area, model.predict(df1[["area"]].values), color="g", label="Linear regression")
plt.legend()

plt.show()
# plt.savefig("./1-house-price.png")





target_area = 3300
prediction_for_target_area = model.predict(np.array([[target_area]]))[0]
print(prediction_for_target_area)


135.78767123*3300 + 180616.43835616432


df2 = pd.read_csv("1-home-price-predict.csv")
df2.tail(3)


model.predict(df2[["area"]]) # get a warning like "UserWarning: X has feature names, but LinearRegression was fitted without feature names warnings.warn"


new_prediction = model.predict(df2[["area"]].values)
new_prediction


df2["price"] = new_prediction
df2


# update `1-home-price-predict.csv` document

# df2.to_csv("home_price_predict.csv", index=False)


plt.figure()
plt.title("House price plotted against house size")
plt.xlabel("House area (square feet)")
plt.ylabel("House price (*1000 USD)")
plt.grid(True)

plt.scatter(df1.area, df1.price, color="red", marker='o', label="Training data")
plt.plot(df1.area, model.predict(df1[["area"]].values), color="g", label="Linear regression")
plt.scatter(df2.area, df2.price, color="blue", marker='o', label="Prediction data")
plt.legend()
plt.tight_layout()
plt.savefig("./1-home-price-predicted.png")








import pickle

# this is binary document
with open("1-home-price-pickle", "wb") as fout:
    pickle.dump(model, fout)


# reuse this model
with open("1-home-price-pickle", "rb") as fin:
    new_model = pickle.load(fin)


print(new_model, model.coef_, model.intercept_)


# make new prediction (as shown in df2 above)

new_model.predict([[3000], [5000]])


# now you can share the `1-model-pickle` file to colleagues


fout = pickle.dumps(model)
model2 = pickle.loads(fout)
print(model2, model2.coef_, model2.intercept_)





# joblib is more efficient than pickle objects 
# that carry large numpy arrays internally as is often the case for 
# fitted scikit-learn estimators

import joblib


joblib.dump(model, "1-home-price-joblib.pkl")


model_joblib = joblib.load("1-home-price-joblib.pkl")


print(model_joblib, model_joblib.coef_, model_joblib.intercept_)








# figure with multiple model parameters

plt.figure()
plt.title("House price plotted against house size")
plt.xlabel("House area (square feet)")
plt.ylabel("House price (*1000 USD)")
plt.grid(True)

plt.scatter(df1.area, df1.price, color="red", marker='o', label="Training data")
plt.plot(df1.area, model.predict(df1[["area"]].values), color="g", label="Linear regression model")

plt.plot(df1.area, df1.area*0.2050 + 81.0, 'b--', label="Model 1")
plt.plot(df1.area, df1.area*0.1850 + 81.0, 'k--', label="Model 2")
plt.plot(df1.area, df1.area*0.1050 + 390., 'c--', label="Model 3")

plt.legend()
plt.tight_layout()

plt.savefig("./1-home-price-trial-models.png")


# `residuals` for `cost function`

plt.figure()
plt.title("House price plotted against house size")
plt.xlabel("House area (square feet)")
plt.ylabel("House price (*1000 USD)")
# plt.grid(True)

residuals = model.predict(df1[["area"]].values)

plt.scatter(df1.area, df1.price, color="red", marker='o', label="Training data")
plt.plot(df1.area, model.predict(df1[["area"]].values), color="g", label="Linear regression model")
plt.vlines(df1.area, ymin=residuals, ymax=df1.price, color='black', linewidth=2, linestyles='dashed')

plt.legend()
plt.tight_layout()
# plt.show()
plt.savefig("./1-home-price-cost-func.png")





ss_res_linear_model = np.mean((model.predict(df1[["area"]]) - df1.price)**2)
ss_res_model_1 = np.mean((df1.area*0.2050 + 81.0 - df1.price)**2)
ss_res_model_2 = np.mean((df1.area*0.1850 + 81.0 - df1.price)**2)
ss_res_model_3 = np.mean((df1.area*0.1050 + 390. - df1.price)**2)

print("The RSS for linear regression model is %.2f." % ss_res_linear_model)
print("The RSS for trial model 1 is %.2f." % ss_res_model_1)
print("The RSS for trial model 2 is %.2f." % ss_res_model_2)
print("The RSS for trial model 3 is %.2f." % ss_res_model_3)











x = df1.area
x_bar = x.mean()
variance = ((x - x_bar)**2).sum() / (x.shape[0] - 1)
print("X_bar = ", x_bar, "\nVariance =", variance)


# Using `var` in numpy for calculating variance
# keyword parameter `ddof=1` is used to set Bessel's correction to calculate sample variance
# why we substract 1 from number of training instances when calculating sample variance?

print("Variance =", np.var(x, ddof=1), "(Calculated from Numpy).")








y = df1.price
y_bar = np.array(y).mean()
covariance = np.multiply((x - x_bar).transpose(), y - y_bar).sum() / (x.shape[0] - 1)
print("Y_bar = ", y_bar, "\nCovariance =", covariance)


# Using `cov` in Numpy for calculating covariance

print("Covariance =", np.cov(x.T, y)[0][1], "(Calculated from Numpy).")








print("w =", covariance / variance)
print("b =", y_bar - covariance / variance * x_bar)

print("\nCompare with model parameters:")
print("w =", model.coef_[0])
print("b =", model.intercept_)





# model.fit(x,y) 
# x can be `x_train` and `x_test`
