# Chapter 5: From Simple to Multiple Linear Regression


- In [Chap. 2, Simple Linear Regression](../chap-02-simple-linear-regression/chap-02-simple-linear-regression.md), we used simple linear regression to model relationship between a single explanatory variable and a continuous response variable.
- In [Chap. 3, Classification and Regression with K-Nearest Neighbors](../chap-03-classification-logistic-regression-with-kNN/chap-03-classification-logistic-regression-with-kNN.md), we introduced kNN and trained classifiers and regressors that use more than one explanatory variable to make predictions.
- In this chapter, we discuss a multiple linear regression, a generalization of simple linear regression that regresses a continuous response variable onto multiple features.
	- we first analytically solve values of parameters that minimize RSS cost function.
	- we then introduce a powerful learning algorithm that can estimate values of parameters that minimize a variety of cost functions, called gradient descent.
	- we then discuss polynomial regression, another special case of multiple linear regression, and learn why increasing modelâ€™s complexity can increase risk that it fails to generalize.







:::danger
:::
